{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9859130,"sourceType":"datasetVersion","datasetId":6050550},{"sourceId":9859378,"sourceType":"datasetVersion","datasetId":6050752},{"sourceId":9859727,"sourceType":"datasetVersion","datasetId":6051021},{"sourceId":9859734,"sourceType":"datasetVersion","datasetId":6051026},{"sourceId":9864593,"sourceType":"datasetVersion","datasetId":6054787}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"79575160-f8d1-4461-b0dd-727b251154c9","cell_type":"code","source":"import pandas as pd\n\nimport numpy as np\n\nimport warnings\n\n\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ebe74ade-740e-4bbd-8883-127ad0b911a4","cell_type":"markdown","source":"**1. K-Means Clustering**","metadata":{}},{"id":"fb513d88-b605-4ae6-8fb9-018398a358df","cell_type":"code","source":"kmeans_df = pd.read_csv('/kaggle/input/final-merged-df/final_merged_df.csv')\n\n\n\nprint(kmeans_df.shape)\n\nkmeans_df.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"77127dff-f096-41c9-bab7-f2ca32ec8cf0","cell_type":"markdown","source":"**Objective 1**\n\n\n\nIdentify regions with sparse grid infrastructure and high population density where wind farms and microgrids could be feasible. This will be determined by analyzing population density projections, income distribution, and wind speed.\n\n\n\nThe **K-Means clustering algorithm** will be used for this objective.","metadata":{}},{"id":"1a757067-1305-403a-8c86-1dcae1eb4765","cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\n\n# Label encode categorical variables\n\n\n\n# Income_Distribution\n\nincome_encoder = LabelEncoder()\n\nkmeans_df['Income_Distribution'] = income_encoder.fit_transform(kmeans_df['Income_Distribution'])\n\n\n\n# Grid_Value\n\ngrid_encoder = LabelEncoder()\n\nkmeans_df['Grid_Value'] = grid_encoder.fit_transform(kmeans_df['Grid_Value'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1b0be40b-8521-4e6d-bc35-1f92f19f0231","cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\n\n# Normalize clustering features\n\n\n\n# Clustering features\n\nfeatures = kmeans_df[['Pop_Density_2020', 'Wind_Speed', 'Latitude', 'Longitude', 'Income_Distribution', 'Grid_Value']]\n\n\n\nscaler = StandardScaler()\n\nscaled_features = scaler.fit_transform(features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ed18d288-81d7-4755-bcba-cf8650043bd1","cell_type":"code","source":"from sklearn.cluster import KMeans\n\nimport matplotlib.pyplot as plt\n\n\n\n# Finding optimal number of clusters\n\ninertia = []\n\nfor k in range(1, 10):\n\n    kmeans = KMeans(n_clusters=k, random_state=0)\n\n    kmeans.fit(scaled_features)\n\n    inertia.append(kmeans.inertia_)\n\n\n\n# Plot elbow graph\n\nplt.plot(range(1, 10), inertia, marker='o')\n\nplt.xlabel('Number of clusters')\n\nplt.ylabel('Inertia')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"78b47a0d-370a-4c52-b936-05a04725d02e","cell_type":"markdown","source":"From the above visualization, using 2 clusters is the optimal choice for segmenting regions based on grid infrastructure sparsity and wind microgrid feasibility.","metadata":{}},{"id":"5bb515cd-41c0-46f5-9173-8141203aae2d","cell_type":"code","source":"# Fit with optimal number of clusters\n\nkmeans = KMeans(n_clusters=2, random_state=42)\n\nkmeans_df['Cluster'] = kmeans.fit_predict(scaled_features)\n\n\n\nkmeans_df[['Latitude', 'Longitude', 'Pop_Density_2020', 'Wind_Speed', 'Grid_Value', 'Income_Distribution', 'Cluster']]\n\nkmeans_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"948aa8f2-890d-4b3d-b06d-a62a9a571848","cell_type":"markdown","source":"**Explore cluster characteristics**","metadata":{}},{"id":"fbe70854-a0e1-47e6-b8c3-310395c173fd","cell_type":"code","source":"# Group by the cluster and calculate the mean of each feature\n\ncluster_summary = kmeans_df.groupby('Cluster')[['Pop_Density_2020', 'Wind_Speed', 'Grid_Value', 'Income_Distribution']].mean()\n\n\n\n# Print cluster summary\n\nprint(cluster_summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"143c3961-7259-4cec-b2fb-9cd99f093989","cell_type":"markdown","source":"**Findings**\n\n\n\n1. *Cluster 0*\n\n\n\n* The areas in cluster 0 are **relatively densely populated** with approximately 95.19 people per square kilometers.\n\n\n\n* The wind conditions in this cluster are slightly stronger than in Cluster 1, with an average speed of 6.14 m/s at 100 meters above ground level. Therefore, these regions are suitable for both **wind farms and wind microgrids**.\n\n\n\n* The grid infrastructure in Cluster 0 is significantly limited, with a value of 0.000096. Given the region's population density, there is clearly **high demand for electricity but limited distribution capacity**.\n\n\n\n* The average income distribution of Cluster 0 is 41.999, indicating that approximately **50% of the population lives below the $2.15 poverty line**. Therefore, this cluster has a higher proportion of people living in poverty.\n\n\n\n\n\n2. *Cluster 1*\n\n\n\n* Compared to Cluster 0, the regions in Cluster 1 have a **slightly lower population density** of 94.17 people per square kilometer.\n\n\n\n* Wind conditions in Cluster 1 are **generally calmer**, with an average wind speed of 5.80 m/s at 100 meters above ground level.\n\n\n\n* Cluster 1 has a **higher presence of grid structures**, with a grid value of 0.5524, indicating that these regions are likely more developed compared to those in Cluster 0 and have higher access to electricity/energy.\n\n\n\n* The average income distribution in Cluster 1 is 21.06, indicating that **only 21.06% of the population in these regions lives below the $2.15 poverty line**.","metadata":{}},{"id":"af793b93-0d91-4c83-b610-df0e0687dcde","cell_type":"code","source":"from sklearn.metrics import davies_bouldin_score\n\n\n\nkmeans_df['Cluster'] = kmeans.labels_\n\ndb_score = davies_bouldin_score(scaled_features, kmeans.labels_)\n\nprint(f\"Davies-Bouldin Index: {db_score}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e33547c6-06f6-48f1-ac49-f8b93f37f13e","cell_type":"markdown","source":"A **Davies-Bouldin Index of 0.73315** indicates that the clusters are relatively compact and distinct.","metadata":{}},{"id":"572d949f-7dbd-46f3-9da3-b396546097ef","cell_type":"code","source":"from sklearn.metrics import calinski_harabasz_score\n\n\n\nkmeans_df['Cluster'] = kmeans.labels_\n\ncalinski_score = calinski_harabasz_score(scaled_features, kmeans.labels_)\n\nprint(f\"Calinski-Harabasz Score: {calinski_score}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"90fa235f-4d44-49fc-8476-959e889e2c61","cell_type":"markdown","source":"A **Calinski-Harabasz Score of 995263.1557610929** data points in each cluster are tightly grouped together, and the clusters themselves are well-separated. \n\n\n\nConclusively, this model has successfully created meaningful clusters with a significant difference between them.","metadata":{}},{"id":"5393a03e-7b04-4f88-ad9a-99a9cf1734e2","cell_type":"code","source":"import pickle\n\n# Assuming your model is named `kmeans_model`\nwith open(\"kmeans_model.pkl\", \"wb\") as f:\n    pickle.dump(kmeans, f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b07468c9-9c1e-4804-9a95-b227fec2138f","cell_type":"code","source":"from flask import Flask, request, jsonify\nimport pickle\nimport numpy as np\n\n# Load pre-trained KMeans model\nwith open(\"kmeans_model.pkl\", \"rb\") as f:\n    kmeans_model = pickle.load(f)\n\napp = Flask(__name__)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.json[\"data\"]\n    data = np.array(data).reshape(1, -1)\n    cluster = kmeans_model.predict(data)\n    return jsonify({\"cluster\": int(cluster[0])})\n\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\", port=5000)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"119fcf13-3f00-4585-b1ed-f3600e31fb2c","cell_type":"markdown","source":"**2. Random Forest**","metadata":{}},{"id":"09d87843-1d01-47c6-b8ba-34038ac66926","cell_type":"markdown","source":"**Objective:** Classify areas based on their suitability for microgrid installation and map regions with high wind speeds to optimize the placement of wind farms and mini-grids in off-grid locations.\r\n\n\n\nClassification targets will be defined and labeled that will classify arreas based on their suitability for wind farm/microgrid installation. These labels will reflect levels of microgrid viability.\n\n\n\nThese labels are based on **renewable energy potential** ['Wind_Speed'] thresholds. The target labels are as follows:\n\n\n\n1. **Viable:** *Areas with wind speed of more than or equal to 5 m/s 100m above ground*.\n\n\n\n2. **Not Viable:** *Areas with wind speed of less than or equal to 5 m/s 100m above ground*.\n","metadata":{}},{"id":"ba0c12ac-c825-4f43-a097-ed16d580d9c7","cell_type":"code","source":"rforest_df = pd.read_csv(r'C:\\Users\\pc\\Documents\\Projects\\Energy_Access_and_Electrification_Planning_in_Kenya\\final_merged_df.csv')\n\nrforest_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"732f6439-ff52-43c6-8a09-41bda6a39819","cell_type":"code","source":"# Define microgrid suitability as binary based on wind speed thresholds\n\nrforest_df['Microgrid_Suitability'] = rforest_df['Wind_Speed'].apply(lambda x: 1 if x >= 5 else 0)\n\n\n\n# Check the result\n\nrforest_df[['Wind_Speed', 'Microgrid_Suitability']].head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8789ed1e-e264-4e3f-b523-17a96febda12","cell_type":"code","source":"rforest_df.Microgrid_Suitability.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dd720bb3-c1f1-4323-bb73-3e7c4f9cec9d","cell_type":"markdown","source":"The y variable is highly imbalanced.","metadata":{}},{"id":"949656f5-62c3-409e-b8fa-d8a0da8b4902","cell_type":"code","source":"# Categorical variables\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\n# Income_Distribution: OHE to avoid imposing any ordinal relationships\n\nrforest_df = pd.get_dummies(rforest_df, columns=['Income_Distribution'], drop_first=False, dtype=int)\n\n\n\n# Grid_Value: Label Encoding since the grid values are discrete categories\n\nlabel_encoder = LabelEncoder()\n\nrforest_df['Grid_Value_Encoded'] = label_encoder.fit_transform(rforest_df['Grid_Value'])\n\n\n\nrforest_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8b0698ee-7ff4-4c2e-9e97-4079ba91ab99","cell_type":"code","source":"# Label X and y\n\n\n\ny = rforest_df['Microgrid_Suitability']\n\nX = rforest_df.drop(columns= ['Microgrid_Suitability', 'Grid_Value'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8016a7fc-456e-48fd-aad9-f6dc832077ab","cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\n\n# Train-test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8b396d35-6b98-41d8-b712-063db5867d25","cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\n\n# Feature scaling\n\nscaler = StandardScaler()\n\n\n\n# Fit only on training data, then transform both training and test data\n\nX_train_scaled = scaler.fit_transform(X_train)\n\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9c8f76b1-5bb9-4fd6-b7ac-d4f73875155c","cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n\n\n# Initialize and train the Random Forest model\n\nmodel = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n\nmodel.fit(X_train_scaled, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ed02d709-8bf5-446c-b256-358e8128c3e9","cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n\n\n# Predict on the test set\n\ny_pred = model.predict(X_test_scaled)\n\n\n\n# Calculate accuracy\n\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Model Accuracy: {accuracy:.2f}\")\n\n\n\n# Classification report for more detailed metrics\n\nprint(\"Classification Report:\")\n\nprint(classification_report(y_test, y_pred))\n\n\n\n# Confusion matrix\n\nprint(\"Confusion Matrix:\")\n\nprint(confusion_matrix(y_test, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"87c8f87e-4c42-4aaa-9fd3-dca3cd8aaf41","cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n\n\n# Initialize the Random Forest model with chosen parameters\n\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\n\n# Perform 5-fold cross-validation\n\ncv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')\n\n\n\nprint(\"Cross-Validation Scores:\", cv_scores)\n\nprint(\"Mean CV Accuracy:\", np.mean(cv_scores))\n\nprint(\"Standard Deviation of CV Accuracy:\", np.std(cv_scores))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0b07672e-2ae8-4d70-9f73-241d295a1b0a","cell_type":"markdown","source":"**3. hdbscan**","metadata":{}},{"id":"78d924ad-5527-4ee2-8655-33402371765e","cell_type":"code","source":"import pandas as pd\n\nhdbscan_df = pd.read_csv(r'C:\\Users\\pc\\Documents\\Projects\\Energy_Access_and_Electrification_Planning_in_Kenya\\final_merged_df.csv')\n\nhdbscan_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dc024fdf-ddd1-458d-a378-2a1933044c9d","cell_type":"code","source":"# One-hot encode 'Income_Distribution'\n\nhdbscan_df = pd.get_dummies(hdbscan_df, columns=['Income_Distribution'], prefix='Income')\n\n\n\nincome_columns = [col for col in hdbscan_df.columns if col.startswith('Income_')]\n\nclustering_data = hdbscan_df[['Pop_Density_2020', 'Wind_Speed', 'Latitude', 'Longitude', 'Grid_Value'] + income_columns]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"672a5ec4-42c6-4f8a-82f1-ef140026669b","cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\n\n# Standardize the data to bring all features to a similar scale\n\nscaler = StandardScaler()\n\nclustering_data_scaled = scaler.fit_transform(clustering_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0143af0f-cc00-4669-8ea2-8ae1551d9839","cell_type":"code","source":"from sklearn.decomposition import PCA\n\n\n\n# Apply PCA to reduce to a manageable number of components\n\npca = PCA(n_components=2)  # Adjust to 2 components for efficient clustering\n\nclustering_data_reduced = pca.fit_transform(clustering_data_scaled)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5fcb5d72-ef74-42d4-ab31-ab9f0c423290","cell_type":"code","source":"import hdbscan\n\n\n\n# Apply HDBSCAN with optimized parameters\n\nhdbscan_clusterer = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=50)  # Adjust min_samples and min_cluster_size as needed\n\nclusters = hdbscan_clusterer.fit_predict(clustering_data_reduced)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"afe0d43d-8a53-4a94-b8f3-33c640e5d73e","cell_type":"code","source":"# Filter out noise points (-1 label in HDBSCAN) before calculating metrics\n\nclustered_data = clustering_data_reduced[clusters != -1]\n\nvalid_clusters = clusters[clusters != -1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"067bc831-3de2-42f9-a8fb-77d61085c2bf","cell_type":"code","source":"# Access cluster stability scores from HDBSCAN\n\nstability_scores = hdbscan_clusterer.probabilities_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1ab8168d-7a17-462a-a5b6-384d73092408","cell_type":"code","source":"from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n\n\n\nif len(set(valid_clusters)) > 1:  # Ensure there's more than one cluster for evaluation\n\n    db_index = davies_bouldin_score(clustered_data, valid_clusters)\n\n    ch_index = calinski_harabasz_score(clustered_data, valid_clusters)\n\n    print(\"Davies-Bouldin Index:\", db_index)\n\n    print(\"Calinski-Harabasz Index:\", ch_index)\n\nelse:\n\n    print(\"Insufficient clusters for evaluation metrics\")\n\n\n\n# Print stability scores and cluster labels\n\nprint(\"Cluster Labels:\", clusters)\n\nprint(\"Cluster Stability Scores:\", stability_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b025c2a7-069f-41bc-ac93-2ffbcb81748f","cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Visualize clustering results\n\nplt.figure(figsize=(10, 6))\n\nplt.scatter(clustering_data_reduced[:, 0], clustering_data_reduced[:, 1], c=clusters, cmap='viridis', s=5)\n\nplt.colorbar(label='Cluster')\n\nplt.xlabel('PCA Component 1')\n\nplt.ylabel('PCA Component 2')\n\nplt.title('HDBSCAN Clustering Results on Full Dataset')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b264a720-cb15-4c9b-80ea-7d7d77607af6","cell_type":"markdown","source":"**4. XGBoost**","metadata":{}},{"id":"11018cb9-5c57-495f-ac4f-789a7138718f","cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"940383a0-ce30-4549-a750-6a9d3054f718","cell_type":"code","source":"xgboost_df = kmeans_df.copy()\nprint(xgboost_df.shape)\nprint(xgboost_df.info())\nxgboost_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4868140c-8f4f-44db-9923-5ff3e683a7da","cell_type":"code","source":"\nX = xgboost_df.drop('Cluster', axis=1)  \ny = xgboost_df['Cluster']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"73688bff-145b-45a1-b5b0-b3b1eecc8855","cell_type":"code","source":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fd71ab7b-496c-4aab-82b4-52e7da0f2e8b","cell_type":"code","source":"\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n\n\n\nxgb_model.fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e609256c-8684-4032-ab54-c28572c6c84f","cell_type":"markdown","source":"**Cross-Validation**","metadata":{}},{"id":"ee8f4c20-41e4-4958-b291-3c71569dfc19","cell_type":"code","source":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom xgboost import XGBClassifier\n\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\ncv_score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')\nprint(f\"Cross-validation Accuracy: {cv_score.mean():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"76976bf4-954e-49ba-b714-d885aac1ae16","cell_type":"markdown","source":"**HyperParameter Tuning**","metadata":{}},{"id":"60d3d643-2d6f-429d-8505-bdb7132ab8b0","cell_type":"code","source":"param_grid = {\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 200],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.7, 0.8, 0.9]\n}\n\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\nprint(f\"Best Parameters: {grid_search.best_params_}\")\nprint(f\"Best Cross-validation Score: {grid_search.best_score_:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7a924e73-19b2-49ec-97d6-10e52d1b6046","cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Predict the labels for the test set\ny_pred = model.predict(X_test)\n\n# Convert probabilities to the class with the highest probability\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Convert y_test from one-hot to single-label format\ny_test_classes = np.argmax(y_test, axis=1)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test_classes, y_pred_classes)\n\n# Plot the confusion matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"04188764-5c12-42e9-9c13-c845a87ab2ed","cell_type":"code","source":"neuralnetwork_df = kmeans_df.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5c8df9f4-a4c1-40e5-8ef6-6e2c8d5a7c60","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\n\nneuralnetwork_df = kmeans_df.copy()\n\nX = neuralnetwork_df.drop(columns=['Cluster'])  # Features\ny = neuralnetwork_df['Cluster']  # Target (Cluster)\n\n# Normalize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# One-hot encode the target variable if it is categorical\ny_one_hot = to_categorical(y)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=42)\n\n# Define the deep learning model\nmodel = Sequential()\n\n# Input Layer (26 features in input)\nmodel.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n\n# Hidden Layers\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\n\n# Output Layer (Number of classes in the target)\nmodel.add(Dense(y_one_hot.shape[1], activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=256, validation_split=0.2)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n\n# You can use the model to predict on new data as well\npredictions = model.predict(X_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0c844148-3e11-4707-90d9-0ff694a67b7a","cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Predict the labels for the test set\ny_pred = model.predict(X_test)\n\n# Convert probabilities to the class with the highest probability\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Convert y_test from one-hot to single-label format\ny_test_classes = np.argmax(y_test, axis=1)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test_classes, y_pred_classes)\n\n# Plot the confusion matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6b9e9211-6010-44cb-8137-3c80d8fa7d66","cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n\ny_test_classes = np.argmax(y_test, axis=1)\n\n\nprint(classification_report(y_test_classes, y_pred_classes))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cf9c4646-6082-4c98-be3b-75eafedac35a","cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Convert y_test and y_pred from one-hot to single-label format (for binary classification)\ny_test_classes = np.argmax(y_test, axis=1)\ny_pred_prob = y_pred[:, 1]  # Assuming class 1 is the positive class, adjust if needed\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test_classes, y_pred_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"162f8079-a732-4258-8532-8551066d0fb9","cell_type":"code","source":"neuralnetwork_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"746dd079-f94d-4dd4-bb88-2763a061c3c2","cell_type":"code","source":"# Convert Grid_Value to categorical (ordinal encoding)\nneuralnetwork_df['Grid_Value'] = neuralnetwork_df['Grid_Value'].astype('category')\n\n# One-hot encoding for categorical Grid_Value\nneuralnetwork_df = pd.get_dummies(neuralnetwork_df, columns=['Grid_Value'], drop_first=True, dtype='int')  \nneuralnetwork_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e74ebb82-512e-4c17-a819-696e94b004f8","cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nX = neuralnetwork_df.copy()  \ny = neuralnetwork_df['Grid_Value']                 \n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e1ddeab2-6a41-40a1-9b52-12f8b9993803","cell_type":"markdown","source":"**transformation model**","metadata":{}},{"id":"4a39a72e-605f-44ff-b12f-86ab449acb33","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6d9a371f-85ca-44af-8698-32a24ebdf684","cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nX = neuralnetwork_df.copy() \n# Scale the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Define input shape based on the number of features\ninput_dim = X.shape[1]\n\n# Autoencoder model with additional dropout and batch normalization\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(128, activation='relu')(input_layer)\nencoded = BatchNormalization()(encoded)\nencoded = Dropout(0.2)(encoded)\nencoded = Dense(64, activation='relu')(encoded)\nencoded = BatchNormalization()(encoded)\nencoded = Dropout(0.2)(encoded)\nencoded = Dense(16, activation='relu')(encoded)  # Compressed representation\n\ndecoded = Dense(64, activation='relu')(encoded)\ndecoded = BatchNormalization()(decoded)\ndecoded = Dropout(0.2)(decoded)\ndecoded = Dense(128, activation='relu')(decoded)\ndecoded = BatchNormalization()(decoded)\ndecoded = Dropout(0.2)(decoded)\ndecoded = Dense(input_dim, activation='linear')(decoded)\n\n# Define the autoencoder model\nautoencoder = Model(inputs=input_layer, outputs=decoded)\nencoder = Model(inputs=input_layer, outputs=encoded)  # Encoder-only model for transformed data\n\n# Compile the model with a lower learning rate\nautoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n# Train the autoencoder with adjusted batch size\nhistory = autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=64, validation_split=0.2)\n\n# Generate transformed data using the encoder\nX_transformed = encoder.predict(X_scaled)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"172ad9bd-ef54-4190-8402-d1b632799312","cell_type":"code","source":"neuralnetwork_df['Grid_Value'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1a8dd5f6-3033-455c-b8a7-74ed98466f51","cell_type":"code","source":"kmeans_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"82ebe832-e7d4-41f5-bd18-dd9fde9d48ec","cell_type":"code","source":"neuralnetwork_df = pd.get_dummies(neuralnetwork_df, columns=['Income_Distribution'], drop_first=True, dtype='int')  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"514b9106-713c-4e50-ae8d-fc78480a5588","cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.regularizers import l2\n\n# Example model architecture with regularization, dropout, and batch normalization\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(1)  # Adjust output layer as per your target variable\n])\n\n# Compile model\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# Early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Fit model with early stopping\nhistory = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val),\n                    batch_size=32, callbacks=[early_stopping])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"edcb158c-647a-4461-b5b2-a1c35da1461b","cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.regularizers import l2\n\n# Example model architecture with regularization, dropout, and batch normalization\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(1)  # Adjust output layer as per your target variable\n])\n\n# Compile model\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# Early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Fit model with early stopping\nhistory = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val),\n                    batch_size=32, callbacks=[early_stopping])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9200bed0-ed64-4f2c-afc3-1d73478c7f9c","cell_type":"code","source":"pip install Flask scikit-learn joblib gunicorn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"63da5459-e5da-429f-8d13-7404ae486b2a","cell_type":"code","source":"from sklearn.cluster import KMeans\nimport joblib\n\n\njoblib.dump(kmeans, 'kmeans_model.pkl')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c36ac634-29f1-4d72-8aa0-b135c6ab74e0","cell_type":"code","source":"from flask import Flask, request, jsonify\nimport joblib\nimport numpy as np\n\napp = Flask(__name__)\n\n# Load the saved KMeans model\nmodel = joblib.load(\"kmeans_model.pkl\")\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # Get data from POST request\n    data = request.get_json(force=True)\n    \n    # Assuming input data is in the form of a 2D list\n    data = np.array(data['input'])\n    \n    # Predict cluster\n    predictions = model.predict(data)\n    \n    # Return predictions as JSON response\n    return jsonify({'predictions': predictions.tolist()})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}